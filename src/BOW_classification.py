# -*- coding: utf-8 -*-
"""IMDB dataset classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KAllYMJzmVoeQLdKpI8SWPK7V0X-OyRU

# **Sentiment Analysis on IMDB Movie Reviews**

# Kaggle API Setup and Dataset Download in Google Colab

This guide provides detailed instructions on how to set up the Kaggle API and download datasets directly into a Google Colab notebook. These steps are essential for accessing Kaggle's vast repository of datasets for data science and machine learning projects.

## Prerequisites

Before beginning, ensure you have a Kaggle account. If you do not have one, sign up at [Kaggle](https://www.kaggle.com).


### Step 1: Install Kaggle Library

First, we install the Kaggle library using pip. This library is essential for interacting with Kaggle's API to download datasets and participate in competitions.

```python
!pip install -q kaggle
```
"""

!pip install -q kaggle

"""Step 2: Upload Kaggle API Key
-----------------------------

To use Kaggle's API, you need an API key, which is a JSON file (`kaggle.json`) obtained from your Kaggle account. To download this file:

1.  Go to your Kaggle account settings.
2.  Scroll to the "API" section and click "Create New API Token".
3.  This will download the `kaggle.json` file to your computer.

Once you have the `kaggle.json` file, you need to upload it to Colab:
"""

from google.colab import files

files.upload()

"""Step 3: Set Up Kaggle API Key
-----------------------------

After uploading the API key, set it up by moving it to the `.kaggle` directory in your home folder. This step ensures that the Kaggle API can access the key for authentication:
"""

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

"""Step 4: Download Dataset
------------------------

With the Kaggle API set up, you can now download datasets. In this example, we'll download the IMDB Dataset of 50K Movie Reviews, a dataset useful for natural language processing and sentiment analysis tasks:
"""

!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
import re

"""Download `punkt` and `stopwords` models using nltk library"""

nltk.download('punkt')
nltk.download('stopwords')

pd.set_option('display.max_colwidth', None)

"""Step 5: Data Preprocessing
------------------

*   **Reading the Dataset**: Load the dataset into a pandas DataFrame for manipulation.
*   **Exploratory Data Analysis (EDA)**: Analyze the dataset to understand its structure, checking for class imbalance, and identifying any missing values.
*   **Text Preprocessing**: Clean and preprocess the review texts by converting to lowercase, removing special characters, and filtering out stopwords.

"""

df = pd.read_csv("/content/imdb-dataset-of-50k-movie-reviews.zip")

df.columns

df['sentiment'].value_counts()

df.shape

df.isna().sum()

df['review'] = df['review'].str.lower()

"""### Step 6: Text Preprocessing

Normalize text data by converting to lowercase, removing special characters, and filtering out stopwords.
"""

stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    text = re.sub(r'\W+', ' ', text)
    # Remove extra whitespaces
    text = re.sub(r'\s+', ' ', text)
    words = word_tokenize(text)
    words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]
    return ' '.join(words)

df['processed_review'] = df['review'].apply(preprocess_text)

X_train, X_test, y_train, y_test = train_test_split(df['processed_review'], df['sentiment'], test_size=0.3, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""Step 7.1: Feature Extraction and Model Training
--------------------------

1.   ### Bag of Words (BoW)
> Indented block Transform the preprocessed text into numerical formats using CountVectorizer for model training.

2.   ### Naive Bayes Classifier
> Indented block Train a Naive Bayes classifier using both BoW and TF-IDF features. Evaluate the model based on accuracy and provide a classification report.
"""

vectorizer = CountVectorizer()
X_train_bow = vectorizer.fit_transform(X_train)
X_test_bow = vectorizer.transform(X_test)

classifier = MultinomialNB()
classifier.fit(X_train_bow, y_train)

predictions = classifier.predict(X_test_bow)

accuracy = accuracy_score(y_test, predictions)
classification_rep = classification_report(y_test, predictions)

print("Accuracy:", accuracy)
print("\nClassification Report:\n", classification_rep)

